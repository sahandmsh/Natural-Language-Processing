{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66NG10faxREV",
        "outputId": "d051cc12-9a0d-42e7-ad6c-68558c08fd9e"
      },
      "id": "66NG10faxREV",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3ee9bf34-5dde-4537-9787-ded9040f296c",
      "metadata": {
        "id": "3ee9bf34-5dde-4537-9787-ded9040f296c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from google import genai\n",
        "from google.colab import drive\n",
        "from google.genai import types\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import argparse\n",
        "import faiss\n",
        "import heapq\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5f228eac-6231-4419-b500-b87653379a7e",
      "metadata": {
        "id": "5f228eac-6231-4419-b500-b87653379a7e"
      },
      "outputs": [],
      "source": [
        "def retrieve_squad_dataset(samples_count: int = 5000, random_sampling: bool = False, dataset_split: str = \"train\"):\n",
        "    \"\"\" Loads squad dataset and resturns expected number of samples\n",
        "    Args:\n",
        "        samples_count (int): number of samples\n",
        "        random_sampling (bool): return random samples if True\n",
        "        dataset_split (str): select with part of dataset to use (e.g., train, test)\n",
        "    returns:\n",
        "        list: a list of sampled items\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"squad\", split = dataset_split)\n",
        "    if random_sampling:\n",
        "        dataset = dataset.shuffle()\n",
        "    dataset = dataset.select(range(min(samples_count, len(dataset))))\n",
        "    sampled_data = [\n",
        "        {'id': item['id'],\n",
        "         'question': item['question'],\n",
        "         'context':item['context'],\n",
        "         'answers':item['answers']} for item in dataset\n",
        "    ]\n",
        "    return sampled_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGCorpusManager:\n",
        "  \"\"\"\n",
        "  Manages a Retrieval-Augmented Generation (RAG) corpus.\n",
        "\n",
        "  This class handles text normalization, deduplication, chunking, embedding generation,\n",
        "  and FAISS index management for efficient document retrieval in RAG systems.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, sentence_transformer: SentenceTransformer, max_data_chunk_len: int = 300, data_chunk_stride: int = 75):\n",
        "    \"\"\"\n",
        "    Initializes the RAGCorpusManager.\n",
        "\n",
        "    Args:\n",
        "        hugging_face_token (str): Authentication token for accessing Hugging Face models.\n",
        "        serntence_transformer (SentenceTransformer): The sentence transformer model from Hugging Face.\n",
        "            used for embedding generation. Defaults to \"all-MiniLM-L6-v2\".\n",
        "        max_data_chunk_len (int, optional): Maximum number of words per chunk. Defaults to 300.\n",
        "        data_chunk_stride (int, optional): Overlap between consecutive chunks in words. Defaults to 75.\n",
        "    \"\"\"\n",
        "    self.raw_dataset = []\n",
        "    self._unique_text_set = set()\n",
        "    self.chunked_data = []\n",
        "    self.chunked_data_metadata = []\n",
        "    self.sentence_embeddings = []\n",
        "    self.faiss_indices = []\n",
        "    self.sentence_transformer = sentence_transformer\n",
        "    self.max_data_chunk_len = max_data_chunk_len\n",
        "    self.data_chunk_stride = data_chunk_stride\n",
        "    if self.max_data_chunk_len <= 0:\n",
        "        raise ValueError(\"Invalid chunk length; it should be a positive value\")\n",
        "    if self.data_chunk_stride < 0 or self.data_chunk_stride >= self.max_data_chunk_len:\n",
        "        raise ValueError(\"Invalid stride value; make sure that 0<= data_chunk_stride < max_data_chunk_len\")\n",
        "\n",
        "\n",
        "  def _normalize_text(self, text: str):\n",
        "    \"\"\"\n",
        "    Normalizes input text by trimming extra spaces and line breaks.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized text with single spaces between words.\n",
        "    \"\"\"\n",
        "    return \" \".join(text.strip().split())\n",
        "\n",
        "\n",
        "  def _find_new_data_entries(self, new_dataset: List[dict]):\n",
        "    \"\"\"\n",
        "    Filters out duplicate entries from the provided dataset.\n",
        "\n",
        "    Args:\n",
        "        new_dataset (list[dict]): List of dataset entries.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: List of unique new entries not already available in the corpus.\n",
        "    \"\"\"\n",
        "    new_unique_dataset = []\n",
        "    for item in new_dataset:\n",
        "      normalized_text = self._normalize_text(item.get('context', \"\"))\n",
        "      if normalized_text and normalized_text not in self._unique_text_set:\n",
        "        self._unique_text_set.add(normalized_text)\n",
        "        new_unique_dataset.append(item)\n",
        "    return new_unique_dataset\n",
        "\n",
        "\n",
        "  def _chunk_data(self, dataset: List[dict]):\n",
        "    \"\"\"\n",
        "    Splits dataset entries into overlapping text chunks based on 'word' count.\n",
        "\n",
        "    Args:\n",
        "        dataset (list[dict]): List of data items\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            list[str]: List of text chunks.\n",
        "            list[dict]: List of metadata dictionaries for each chunk, including:\n",
        "                - 'source_id': ID of the source document.\n",
        "                - 'data_chunk_id': Sequential chunk ID.\n",
        "                - 'start_word_index': Starting word index in the source text.\n",
        "    \"\"\"\n",
        "    data_chunks = []\n",
        "    data_chunks_metadata = []\n",
        "    for item in dataset:\n",
        "        text = item.get('context', \"\")\n",
        "        if not text:\n",
        "            continue\n",
        "        words_list = text.split()\n",
        "        if len(words_list)<=self.max_data_chunk_len:\n",
        "            data_chunks.append(text)\n",
        "            data_chunks_metadata.append({'source_id': item.get('id', \"NA\"), 'data_chunk_id': 0, 'start_word_index': 0})\n",
        "            continue\n",
        "        start_word_index = 0\n",
        "        data_chunk_id = 0\n",
        "        while start_word_index < len(words_list):\n",
        "            chunk = words_list[start_word_index:start_word_index + self.max_data_chunk_len]\n",
        "            data_chunks.append(\" \".join(chunk))\n",
        "            data_chunks_metadata.append({\n",
        "                'source_id': item.get('id', \"NA\"),\n",
        "                'data_chunk_id': data_chunk_id,\n",
        "                'start_word_index': start_word_index\n",
        "            })\n",
        "            if start_word_index + self.max_data_chunk_len >= len(words_list):\n",
        "                break\n",
        "            start_word_index += self.max_data_chunk_len - self.data_chunk_stride\n",
        "            data_chunk_id += 1\n",
        "    return data_chunks, data_chunks_metadata\n",
        "\n",
        "\n",
        "  def _calculate_sentence_embeddings(self, data_chunks: List[str], batch_size: int = 64):\n",
        "    \"\"\"\n",
        "    Computes sentence embeddings for a list of text chunks.\n",
        "\n",
        "    Args:\n",
        "        data_chunks (list[str]): List of text chunks to embed.\n",
        "        batch_size (int, optional): Number of chunks processed per batch. Defaults to 64.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array of embeddings corresponding to each input chunk.\n",
        "    \"\"\"\n",
        "    sentence_embeddings = self.sentence_transformer.encode(\n",
        "        data_chunks,\n",
        "        batch_size = batch_size,\n",
        "        show_progress_bar = True,\n",
        "        convert_to_numpy = True\n",
        "    )\n",
        "    return sentence_embeddings\n",
        "\n",
        "\n",
        "  def _create_update_faiss_index(self, embeddings: np.ndarray):\n",
        "    \"\"\"\n",
        "    Creates or updates the FAISS index with new embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): New embeddings to add to the FAISS index.\n",
        "\n",
        "    Notes:\n",
        "        - Uses cosine similarity (inner product) as the metric.\n",
        "        - Normalizes embeddings before indexing.\n",
        "        - Creates the FAISS index object if it does not already exist.\n",
        "    \"\"\"\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    if not hasattr(self, \"faiss_index\"):\n",
        "      embedding_dim = embeddings.shape[1]\n",
        "      faiss.normalize_L2(embeddings)\n",
        "      self.faiss_index = faiss.IndexFlatIP(embedding_dim)\n",
        "    self.faiss_index.add(embeddings)\n",
        "\n",
        "\n",
        "  def add_update_data_and_index(self, dataset: List[dict]):\n",
        "    \"\"\"\n",
        "    Adds new data entries to the corpus and updates the FAISS index.\n",
        "\n",
        "    This method performs the following steps:\n",
        "      1. Removes duplicate entries.\n",
        "      2. Splits texts into overlapping chunks.\n",
        "      3. Computes embeddings for the new chunks.\n",
        "      4. Updates the FAISS index.\n",
        "      5. Extends the internal corpus data structures.\n",
        "\n",
        "    Args:\n",
        "        dataset (list[dict]): New dataset entries to add.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    new_data_entries = self._find_new_data_entries(dataset)\n",
        "    if not new_data_entries:\n",
        "      return\n",
        "    new_data_chunks, new_data_chunks_metadata = self._chunk_data(new_data_entries)\n",
        "    new_embeddings = self._calculate_sentence_embeddings(new_data_chunks)\n",
        "    self._create_update_faiss_index(new_embeddings)\n",
        "    self.raw_dataset.extend(new_data_entries)\n",
        "    self.chunked_data.extend(new_data_chunks)\n",
        "    self.chunked_data_metadata.extend(new_data_chunks_metadata)"
      ],
      "metadata": {
        "id": "rCK_wopzXIS1"
      },
      "id": "rCK_wopzXIS1",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGAnswerRetrieval:\n",
        "  \"\"\"\n",
        "  Handles end-to-end answer retrieval in a Retrieval-Augmented Generation (RAG) system.\n",
        "\n",
        "  This class integrates three main components:\n",
        "    1. A **bi-encoder** (via FAISS index) for fast passage retrieval.\n",
        "    2. A **cross-encoder** for re-ranking retrieved passages by semantic relevance.\n",
        "    3. A **generative model** (e.g., Gemini) for producing natural language answers\n",
        "       conditioned on the retrieved context.\n",
        "  \"\"\"\n",
        "  def __init__(self, tokenizer, cross_encoder, generative_model):\n",
        "    \"\"\"\n",
        "        Initializes the RAGAnswerRetrieval class with tokenizer, cross-encoder, and generative model.\n",
        "\n",
        "        Args:\n",
        "            tokenizer (AutoTokenizer): Tokenizer for the cross-encoder.\n",
        "            cross_encoder (AutoModelForSequenceClassification): Pretrained cross-encoder model.\n",
        "            generative_model: Gemini generative model with prompt (str) as input and generated text (str) as output.\n",
        "        \"\"\"\n",
        "    self._tokenizer = tokenizer\n",
        "    self._cross_encoder = cross_encoder\n",
        "    self._generative_model = generative_model\n",
        "\n",
        "\n",
        "  def _merge_passage_chunks_and_scores(self, chunks_list):\n",
        "    \"\"\"\n",
        "    Merges overlapping text chunks and computes their average relevance score.\n",
        "\n",
        "    Args:\n",
        "        chunks_list (list[tuple]): List of tuples, each containing:\n",
        "            - start_word_index (int): Start position of the chunk in the original text.\n",
        "            - chunk_text (str): Chunk text content.\n",
        "            - score (float): Relevance score assigned by the cross-encoder.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            str: Merged text string from the chunks.\n",
        "            float: Average relevance score across the merged chunks.\n",
        "    \"\"\"\n",
        "    merged_text_list = []\n",
        "    chunk_score = 0\n",
        "    for i in range(len(chunks_list)):\n",
        "        if i < len(chunks_list) - 1 and chunks_list[i+1][0] <= chunks_list[i][0] + len(chunks_list[i][1]) - 1:\n",
        "            merged_text_list.append(chunks_list[i][1][ : chunks_list[i+1][0] - chunks_list[i][0]])\n",
        "        else:\n",
        "            merged_text_list.append(chunks_list[i][1])\n",
        "        chunk_score += chunks_list[i][2]\n",
        "    return \" \".join(merged_text_list), chunk_score/len(chunks_list)\n",
        "\n",
        "\n",
        "  def _biencoder_find_top_similar_items(self, query, top_k, rag_corpus: RAGCorpusManager):\n",
        "    \"\"\"\n",
        "    Retrieves top-k semantically similar passages using the FAISS bi-encoder index.\n",
        "\n",
        "    Args:\n",
        "        query (str): Query text to search for.\n",
        "        top_k (int): Number of top results to retrieve.\n",
        "        rag_corpus (RAGCorpusManager): Corpus manager containing FAISS index and embeddings.\n",
        "\n",
        "    Returns:\n",
        "        list[int]: Indices of the top-k most similar passage chunks in the corpus.\n",
        "    \"\"\"\n",
        "    query_embedding = rag_corpus.sentence_transformer.encode([query], convert_to_numpy = True)\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    _, indices = rag_corpus.faiss_index.search(query_embedding, top_k)\n",
        "    return indices[0].tolist()\n",
        "\n",
        "\n",
        "  def _cross_encoder_find_top_similar_items(self, query: str, rag_corpus: RAGCorpusManager, passage_chunk_indices: List[int], top_k: int = 5, batch_size: int = 64, max_length: int = 512):\n",
        "    \"\"\"\n",
        "    Re-ranks retrieved passages using a cross-encoder for more precise semantic relevance.\n",
        "\n",
        "    Args:\n",
        "        query (str): Input query text.\n",
        "        rag_corpus (RAGCorpusManager): Corpus manager containing chunked data and metadata.\n",
        "        passage_chunk_indices (list[int]): Indices of passage chunks retrieved by the bi-encoder.\n",
        "        top_k (int, optional): Number of top passages to keep after re-ranking. Defaults to 5.\n",
        "        batch_size (int, optional): Number of passage pairs to process in a single batch. Defaults to 64.\n",
        "        max_length (int, optional): Maximum token length per input pair. Defaults to 512.\n",
        "\n",
        "    Returns:\n",
        "        list[tuple]: List of tuples containing merged passage text and corresponding average score,\n",
        "        sorted by descending relevance.\n",
        "    \"\"\"\n",
        "\n",
        "    self._cross_encoder.eval()\n",
        "    scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start in range(0, len(passage_chunk_indices), batch_size):\n",
        "            batch_indices = passage_chunk_indices[start:start + batch_size]\n",
        "            batch_passages = [rag_corpus.chunked_data[i] for i in batch_indices]\n",
        "            batch_pairs = list(zip([query] * len(batch_passages), batch_passages))\n",
        "            inputs = self._tokenizer(batch_pairs, padding=True, truncation='only_second', max_length=max_length, return_tensors=\"pt\")\n",
        "            outputs = self._cross_encoder(**inputs)\n",
        "            logits = outputs.logits\n",
        "            batch_scores = logits.view(-1).tolist()\n",
        "            scores.extend(zip(batch_indices, batch_scores))\n",
        "    most_related_items_index_score_pairs = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "    top_chunks_idx_to_metadata = {}\n",
        "    top_k_results = []\n",
        "    for index, score in most_related_items_index_score_pairs:\n",
        "      passage_id = rag_corpus.chunked_data_metadata[index]['source_id']\n",
        "      start_word_index = rag_corpus.chunked_data_metadata[index]['start_word_index']\n",
        "      top_chunks_idx_to_metadata.setdefault(passage_id, [])\n",
        "      heapq.heappush(top_chunks_idx_to_metadata[passage_id], (start_word_index, rag_corpus.chunked_data[index], score))\n",
        "    for key, chunks_list in top_chunks_idx_to_metadata.items():\n",
        "            top_k_results.append(self._merge_passage_chunks_and_scores(chunks_list))\n",
        "    return top_k_results\n",
        "\n",
        "\n",
        "  def find_query_response(self, rag_corpus: RAGCorpusManager, query: str, biencodr_top_k: int = 100, cross_encoder_top_k = 5, cross_encoder_batch_size: int = 16, max_cross_encoder_token_length: int = 512):\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant context and generates a natural language response for a query.\n",
        "\n",
        "    This method performs the following steps:\n",
        "      1. Uses the bi-encoder FAISS index to find top-k relevant passages.\n",
        "      2. Re-ranks those passages using a cross-encoder for better precision.\n",
        "      3. Merges overlapping chunks.\n",
        "      4. Constructs a prompt combining the query and top contexts.\n",
        "      5. Generates a response using the generative model.\n",
        "\n",
        "    Args:\n",
        "        rag_corpus (RAGCorpusManager): Corpus manager containing documents, embeddings, and metadata.\n",
        "        query (str): User query string.\n",
        "        biencodr_top_k (int, optional): Number of passages retrieved by the bi-encoder. Defaults to 100.\n",
        "        cross_encoder_top_k (int, optional): Number of passages kept after cross-encoder re-ranking. Defaults to 5.\n",
        "        cross_encoder_batch_size (int, optional): Batch size for cross-encoder inference. Defaults to 16.\n",
        "        max_cross_encoder_token_length (int, optional): Maximum token length for cross-encoder inputs. Defaults to 512.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            str: Combined context text used for response generation.\n",
        "            str: Generated answer from the generative model.\n",
        "    \"\"\"\n",
        "    biencoder_top_passage_chunk_indices = self._biencoder_find_top_similar_items(query, biencodr_top_k, rag_corpus)\n",
        "    cross_encoder_top_similar_items = self._cross_encoder_find_top_similar_items(query, rag_corpus, biencoder_top_passage_chunk_indices, cross_encoder_top_k, cross_encoder_batch_size, max_cross_encoder_token_length)\n",
        "    context = \"\\nContext: \".join([passage_and_score[0] for passage_and_score in cross_encoder_top_similar_items])\n",
        "    prompt = f\"Question: {query}\\nContext: {context}\"\n",
        "    response = self._generative_model(prompt)\n",
        "    return context, response"
      ],
      "metadata": {
        "id": "GfisyXxNxXsC"
      },
      "id": "GfisyXxNxXsC",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hf_models(hugging_face_token: str, tokenizer_model: str =\"cross-encoder/ms-marco-MiniLM-L-6-v2\", cross_encoder_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\", sentence_embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
        "  \"\"\"\n",
        "  Loads pretrained tokenizer, cross-encoder, and sentence embedding models from Hugging Face.\n",
        "\n",
        "  This utility function initializes and returns all major model components required for a\n",
        "  Retrieval-Augmented Generation (RAG) pipeline, including:\n",
        "    - A tokenizer for text encoding.\n",
        "    - A cross-encoder for reranking retrieved passages.\n",
        "    - A SentenceTransformer model for bi-encoder-style sentence or chunk embeddings.\n",
        "\n",
        "  Args:\n",
        "      hugging_face_token (str): Authentication token for accessing private or gated Hugging Face models.\n",
        "      tokenizer_model (str, optional): Model name or path for the tokenizer.\n",
        "          Defaults to \"cross-encoder/ms-marco-MiniLM-L-6-v2\".\n",
        "      cross_encoder_model (str, optional): Model name or path for the cross-encoder.\n",
        "          Defaults to \"cross-encoder/ms-marco-MiniLM-L-6-v2\".\n",
        "      sentence_embedding_model (str, optional): Model name or path for the SentenceTransformer.\n",
        "          Defaults to \"all-MiniLM-L6-v2\".\n",
        "\n",
        "  Returns:\n",
        "      tuple:\n",
        "          tokenizer (AutoTokenizer): Pretrained tokenizer compatible with the cross-encoder model.\n",
        "          cross_encoder (AutoModelForSequenceClassification): Cross-encoder model for scoring query–passage pairs.\n",
        "          sentence_transformer (SentenceTransformer): Bi-encoder model for embedding text chunks or sentences.\n",
        "  \"\"\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_model, token = hugging_face_token)\n",
        "  cross_encoder = AutoModelForSequenceClassification.from_pretrained(cross_encoder_model, token = hugging_face_token)\n",
        "  sentence_transformer = SentenceTransformer(sentence_embedding_model, token = hugging_face_token)\n",
        "  return tokenizer, cross_encoder, sentence_transformer"
      ],
      "metadata": {
        "id": "hcy-7MIg_LEU"
      },
      "id": "hcy-7MIg_LEU",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_gemini_model(google_api_key: str, config, model_name: str = \"gemini-2.5-flash\"):\n",
        "    \"\"\"\n",
        "    Loads a Gemini generative model for text generation.\n",
        "\n",
        "    Args:\n",
        "        google_api_key (str): API key for the Google Generative AI service.\n",
        "        config (dict): Model configuration (e.g., temperature, max_output_tokens).\n",
        "        model_name (str): Name of the generative model to load.\n",
        "\n",
        "    Returns:\n",
        "        Callable: A function `generate(prompt: str)` that produces text\n",
        "        for a given prompt using the specified model.\n",
        "    \"\"\"\n",
        "    client = genai.Client(api_key = google_api_key)\n",
        "    def generate(prompt: str):\n",
        "        response = client.models.generate_content(model = model_name, contents= prompt, config = config)\n",
        "        return response.text\n",
        "    return generate"
      ],
      "metadata": {
        "id": "rhmxLQQ2fnlW"
      },
      "id": "rhmxLQQ2fnlW",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Following is an example use case of the above RAG implementation**"
      ],
      "metadata": {
        "id": "oirVTTs-T0ix"
      },
      "id": "oirVTTs-T0ix"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. The knowledge base is created by sampling the Squad dataset."
      ],
      "metadata": {
        "id": "xZ__zblEBZvG"
      },
      "id": "xZ__zblEBZvG"
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = retrieve_squad_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiMSWjgFg-2K",
        "outputId": "ca3a5cfd-e223-4952-9880-d5df9d5a29f3"
      },
      "id": "fiMSWjgFg-2K",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load the tokenizer, cross encoder, and sentence transformer (for biencoder)."
      ],
      "metadata": {
        "id": "Qh_HFnGlBmTr"
      },
      "id": "Qh_HFnGlBmTr"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "tokenizer, cross_encoder, sentence_transformer = load_hf_models(hugging_face_token = userdata.get('SECRET_HF_TOKEN'))\n"
      ],
      "metadata": {
        "id": "_27JlGzExXz4"
      },
      "id": "_27JlGzExXz4",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create the rag_corpus object and index the given knowledge base after chunking its text and calculate sentence embeddings."
      ],
      "metadata": {
        "id": "K4rsiXIPB2MX"
      },
      "id": "K4rsiXIPB2MX"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_corpus = RAGCorpusManager(sentence_transformer = sentence_transformer)\n",
        "rag_corpus.add_update_data_and_index(knowledge_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "08a1dce1f79642458097a43c15dff854",
            "7689197d77c64f709a313d6f1d86bc06",
            "1ed4d42c42f24b0dbeb4948eb837d87b",
            "f431cdd770bf4a4a9e9b330df0ebfbf6",
            "18862205aa4646fbb04fcf2e66be5dad",
            "59a00910630143f0a7751413b2eec7a1",
            "c3f803e4819a45c79686a8bdee01da03",
            "003f341340f84b1a98cae4f862bcf500",
            "f98dbfd0204045be8bd3ec0021512870",
            "9cbc385abcbb4454b94df802da90d71b",
            "1436e910ba5b4a129502570b836ad872"
          ]
        },
        "id": "PaBI667MhBNo",
        "outputId": "ad8d28f2-b6a8-4032-afc3-709918b32975"
      },
      "id": "PaBI667MhBNo",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08a1dce1f79642458097a43c15dff854"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. The next step is to load a generative language model."
      ],
      "metadata": {
        "id": "HRIO6sj8CBDp"
      },
      "id": "HRIO6sj8CBDp"
    },
    {
      "cell_type": "code",
      "source": [
        "config = types.GenerateContentConfig(\n",
        "    temperature=0.5,\n",
        "    max_output_tokens= 1024,\n",
        "    system_instruction=\"Return plain text only, no markdown, no special characters. Only used provided 'contexts' to generate response. Always Return 'I don't know' if the response is not found in the context.\"\n",
        ")\n",
        "generative_model = load_gemini_model(google_api_key = userdata.get('SECRET_GOOGLE_API_KEY'), config = config)"
      ],
      "metadata": {
        "id": "LMvVePeUg3b4"
      },
      "id": "LMvVePeUg3b4",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Then, the rag_response_retriever object is created."
      ],
      "metadata": {
        "id": "i_RLRxxPCKFg"
      },
      "id": "i_RLRxxPCKFg"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_response_retriever = RAGAnswerRetrieval(tokenizer, cross_encoder, generative_model)"
      ],
      "metadata": {
        "id": "Uq1PxaFjTRm0"
      },
      "id": "Uq1PxaFjTRm0",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. The retriever is now ready to be used. Following are a few example use cases of finding the answer of the given question using the developed RAG pipeline."
      ],
      "metadata": {
        "id": "avRFfS9xCPtP"
      },
      "id": "avRFfS9xCPtP"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_response_retriever.find_query_response(rag_corpus, \"Where is Notre Dame?\")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fQz8g6b_SOpK",
        "outputId": "688b197a-5f0d-4fe5-8e07-6c250e6c5941"
      },
      "id": "fQz8g6b_SOpK",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The University of Notre Dame du Lac is located adjacent to South Bend, Indiana, in the United States.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_response_retriever.find_query_response(rag_corpus, \"Where is Indiana?\")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oZA4x1ZrUljk",
        "outputId": "51353d40-4f4e-4169-caed-f22f73d93d24"
      },
      "id": "oZA4x1ZrUljk",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Indiana is in the United States.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_response_retriever.find_query_response(rag_corpus, \"Who is Notre Dome President?\")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wwvUBbJgUznV",
        "outputId": "6ba2291b-15b8-4178-f7d3-87f63568d6dc"
      },
      "id": "wwvUBbJgUznV",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Since 2005, Notre Dame has been led by John I. Jenkins, C.S.C., the 17th president of the university.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_response_retriever.find_query_response(rag_corpus, \"Who is Notre Dome President in 2008?\")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f-IQXl3DDyXa",
        "outputId": "d2e0e810-042f-44f3-bb71-8f891e3b9f34"
      },
      "id": "f-IQXl3DDyXa",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'John I. Jenkins'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note:** The RAG pipeline may fail to find an answer to a question in the following cases:\n",
        "\n",
        "* The answer does not exist in the underlying knowledge base or database.\n",
        "\n",
        "* The RAG retriever fails to retrieve the relevant context for the query.\n",
        "\n",
        "* The generative model fails to produce the correct answer even when the relevant context is retrieved."
      ],
      "metadata": {
        "id": "7ZS1K2RaCdEw"
      },
      "id": "7ZS1K2RaCdEw"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_response_retriever.find_query_response(rag_corpus, \"Who is 16th president of Notre Dome?\")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mSgN-0Y4VFMA",
        "outputId": "e831e7fd-9009-4726-cd44-f92a0ba8f6a8"
      },
      "id": "mSgN-0Y4VFMA",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    },
    "7689197d77c64f709a313d6f1d86bc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59a00910630143f0a7751413b2eec7a1",
            "placeholder": "​",
            "style": "IPY_MODEL_c3f803e4819a45c79686a8bdee01da03",
            "value": "Batches: 100%"
          }
        },
        "1ed4d42c42f24b0dbeb4948eb837d87b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_003f341340f84b1a98cae4f862bcf500",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f98dbfd0204045be8bd3ec0021512870",
            "value": 13
          }
        },
        "f431cdd770bf4a4a9e9b330df0ebfbf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cbc385abcbb4454b94df802da90d71b",
            "placeholder": "​",
            "style": "IPY_MODEL_1436e910ba5b4a129502570b836ad872",
            "value": " 13/13 [01:33&lt;00:00,  3.41s/it]"
          }
        },
        "18862205aa4646fbb04fcf2e66be5dad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59a00910630143f0a7751413b2eec7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3f803e4819a45c79686a8bdee01da03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "003f341340f84b1a98cae4f862bcf500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f98dbfd0204045be8bd3ec0021512870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cbc385abcbb4454b94df802da90d71b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1436e910ba5b4a129502570b836ad872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
