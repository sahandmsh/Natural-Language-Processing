{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9f06ec-855e-403b-b6da-89bd60e4a865",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59c396a0-4881-4fa7-8bac-f533b2164efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95e6a8-04f7-41e1-884c-7ba06d0d0fdc",
   "metadata": {},
   "source": [
    "Load a dataset of 750 words used to train the RNN-based character level language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b95fedce-8150-4da3-8f2c-8dd90a5b243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_of_words.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e75fcf-ed4f-40f7-9679-e054bdc43799",
   "metadata": {},
   "source": [
    "The vocabulary is built using the characters available in the given dataset\n",
    "\n",
    "The dataset is modified such that start (<span style=\"color:red\">\"\\<s\\>\"</span>) and end (<span style=\"color:red\">\"\\</s\\>\"</span>) tokens are wrapped around each word\n",
    "\n",
    "The tokens are also added to the vocabulary\n",
    "\n",
    "The dataset is converted to a string so that can be used in the implemented <span style=\"color:red\">CharecterLevelRNN</span> class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7a94e2-5d74-4941-b1bc-9141a56f1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(' '.join(data)))\n",
    "vocab.sort()\n",
    "vocab.extend([\"<s>\", \"</s>\"]) # word start and end tokens\n",
    "data = [f\"<s>{word}</s>\" for word in data]\n",
    "data = ''.join(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85c5fd-c8b1-4afd-b116-6170a3915c96",
   "metadata": {},
   "source": [
    "The CharecterLevelRNN class includes the implementation of the language model together with methods used in training the model and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0be2dc2-0b6c-485f-b80c-f9278f38d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharecterLevelRNN(nn.Module):\n",
    "    \"\"\" RNN-based character level language model\n",
    "        The developed model generates words one letter at a time\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, embedding_size, hidden_layer_size):\n",
    "        nn.Module.__init__(self)\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_size)        \n",
    "        self.rnn = nn.RNN(embedding_size, hidden_layer_size, batch_first=True)\n",
    "        self.fully_connected = nn.Linear(hidden_layer_size, self.vocab_size)\n",
    "        self.char_to_index_map = {vocab[i]:i for i in range(len(vocab))}\n",
    "        self.index_to_char_map = vocab\n",
    "        self.start_token = \"<s>\"\n",
    "        self.end_token = \"</s>\"\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden = None):\n",
    "        \"\"\" Neural network forward pass\n",
    "        Args:\n",
    "            x(torch.Tensor): input data\n",
    "            hidden(torch.Tensor): hidden state\n",
    "        Returns:\n",
    "            tuple: logits and the last calculated hidden state\n",
    "            \n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        logits = self.fully_connected(output)\n",
    "        return logits, hidden\n",
    "        \n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\" Converts string to a list of characters' indices in the vocabulary\n",
    "        Args:\n",
    "            text (str): text to be processed\n",
    "        Returns:\n",
    "            list: list of indices\n",
    "        \"\"\"\n",
    "        i, indices = 0, []\n",
    "        while i < len(text):\n",
    "            for token in [self.start_token, self.end_token]:\n",
    "                if text.startswith(token, i):\n",
    "                    indices.append(self.char_to_index_map[token])\n",
    "                    i += len(token)\n",
    "                    break\n",
    "            else:\n",
    "                ch = text[i]\n",
    "                indices.append(self.char_to_index_map[ch])\n",
    "                i += 1\n",
    "        return indices\n",
    "\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\" Converts indices to corresponding characters\n",
    "        Args:\n",
    "            indices(list): list of indices\n",
    "        Returns:\n",
    "            str: Decoded string\n",
    "        \"\"\"\n",
    "        return ''.join([self.index_to_char_map[index] for index in indices])\n",
    "\n",
    "    \n",
    "    def __generate_batch(self, data, seq_length = 25, batch_size = 64):\n",
    "        \"\"\" Randomly samples data and generate a batch\n",
    "        Args:\n",
    "            data: dataset to be sampled\n",
    "            seq_length(int): length of each sampled sequence\n",
    "            batch_size(int): batch size\n",
    "        Returns:\n",
    "            tuple: a batch of data and labels\n",
    "        \"\"\"\n",
    "        batch_start_indices = torch.randint(len(data) - seq_length, (batch_size,))\n",
    "        x = torch.stack([data[i:i+seq_length] for i in batch_start_indices])\n",
    "        y = torch.stack([data[i+1:i+seq_length+1] for i in batch_start_indices])\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "    def train_model(self, data, epochs, optimizer, criterion):\n",
    "        \"\"\" Trains the weigths of the model\n",
    "        Args:\n",
    "            data(str): training data\n",
    "            epochs: number of epochs\n",
    "            optimizer: e.g., torch.optim.Adam\n",
    "            criterion: e.g., nn.torch.CrossEntropyLoss\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        data = torch.tensor(self.encode(data), dtype = torch.long)\n",
    "        for epoch in range(1, epochs+1):\n",
    "            self.train()\n",
    "            x_batch, y_batch = self.__generate_batch(data)\n",
    "            logits, _ = self.forward(x_batch)\n",
    "            loss = criterion(logits.view(-1, self.vocab_size), y_batch.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    def __calculate_hidden_state(self, input_seq):\n",
    "        \"\"\" Calculates hidden state (which is used to generate the next character given a sequence)\n",
    "        Args:\n",
    "            input_seq(torch.Tensor): sequence of input\n",
    "        Returns:\n",
    "            torch.Tensor: last calculated hidden state\n",
    "        \"\"\"\n",
    "        if input_seq.nelement() == 0:\n",
    "            return None\n",
    "        _, hidden = self.forward(input_seq, None)\n",
    "        return hidden.squeeze(0)\n",
    "\n",
    "\n",
    "    def generate_word(self, prefix = None, temperature = 1.0):\n",
    "        \"\"\" Generates a word character by character given a prefix. Uses softmax, and samples a char randomly\n",
    "        Args:\n",
    "            prefix(str): prefix of the expected word\n",
    "            temperature(float): temperature scaling applied to logits \n",
    "                temperature<1 -> more confident sampling (peaky distribution)\n",
    "                temperature=1 -> normal sampling\n",
    "                temperature>1 -> more creative sampling (flatter distribution)\n",
    "        Returns:\n",
    "            str: generated word\n",
    "        \"\"\"\n",
    "        if prefix is None:\n",
    "            prefix = self.start_token\n",
    "        if not prefix.startswith(self.start_token):\n",
    "            prefix = self.start_token + prefix\n",
    "        indices   = self.encode(prefix)\n",
    "        input_seq = torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # shape (1, L)\n",
    "        hidden = self.__calculate_hidden_state(input_seq[:,:-1])\n",
    "        self.eval()\n",
    "        while True:\n",
    "            current_index = input_seq[:,-1]\n",
    "            logit, hidden = self.forward(current_index, hidden)\n",
    "            probs = torch.softmax(logit/temperature, dim=-1)\n",
    "            predicted_index = torch.multinomial(probs, num_samples=1).item()\n",
    "            if self.decode([predicted_index]) == self.end_token:\n",
    "                break\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[predicted_index]])], dim=1)\n",
    "        return(self.decode(input_seq.squeeze().tolist()).replace(self.start_token, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba74ec6-d9cf-4192-b69b-ba8c7f9db89b",
   "metadata": {},
   "source": [
    "Creating an object of the CharecterLevelRNN class (embedding size = 32; hidden layer size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8eb64fb-8a82-4339-bd8e-1b67cd6d0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level_model = CharecterLevelRNN(vocab, 32, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141cd9b9-115b-4769-ad5c-0d78f3210eb9",
   "metadata": {},
   "source": [
    "Defining optimizer and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b39a3047-8345-4185-9f89-25e151a3d3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss: 0.8703\n",
      "Epoch 1000, Loss: 0.4477\n",
      "Epoch 1500, Loss: 0.3565\n",
      "Epoch 2000, Loss: 0.3259\n",
      "Epoch 2500, Loss: 0.3183\n",
      "Epoch 3000, Loss: 0.2896\n",
      "Epoch 3500, Loss: 0.2885\n",
      "Epoch 4000, Loss: 0.2673\n",
      "Epoch 4500, Loss: 0.2757\n",
      "Epoch 5000, Loss: 0.2687\n",
      "Epoch 5500, Loss: 0.2624\n",
      "Epoch 6000, Loss: 0.2631\n",
      "Epoch 6500, Loss: 0.2490\n",
      "Epoch 7000, Loss: 0.2522\n",
      "Epoch 7500, Loss: 0.2691\n",
      "Epoch 8000, Loss: 0.2569\n",
      "Epoch 8500, Loss: 0.2547\n",
      "Epoch 9000, Loss: 0.2663\n",
      "Epoch 9500, Loss: 0.2532\n",
      "Epoch 10000, Loss: 0.2448\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(char_level_model.parameters(), 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "char_level_model.train_model(data, 10000, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c172ec-367a-4c2c-84e3-0ce1a9dfa150",
   "metadata": {},
   "source": [
    "<span style = \"color:blue\">Example 1:</span> Generating words with suffix <span style=\"color:blue\">co\"</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d40fe22a-2b41-413f-98ff-090cf7f2fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['college', 'color', 'computer', 'computer', 'computer', 'cook', 'cool', 'correct', 'cost', 'course']\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "for _ in range(10):\n",
    "    ans.append(char_level_model.generate_word(\"co\"))\n",
    "print(sorted(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c55162-7c40-4eee-a8b0-068aa53449ba",
   "metadata": {},
   "source": [
    "<span style = \"color:blue\">Example 2:</span> Generating words starting with <span style = \"color:blue\">b</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "741b765b-276b-4031-8429-c676764f4f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bag', 'base', 'begin', 'behind', 'big', 'block', 'boat', 'boy', 'bubllow', 'busy']\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "for _ in range(10):\n",
    "    ans.append(char_level_model.generate_word(\"b\"))\n",
    "print(sorted(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5a12b-ce95-456a-a572-ec1a68ce21de",
   "metadata": {},
   "source": [
    "<span style = \"color:blue\">Example 3:</span> Generating words starting with <span style = \"color:blue\">z</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e94e7d93-a0b2-47ae-a99c-b90fbfc06b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zapper', 'zecour', 'zen', 'zence', 'zeolite', 'zeolite', 'zeppelin', 'zillion', 'zip', 'zoom']\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "for _ in range(10):\n",
    "    ans.append(char_level_model.generate_word(\"z\"))\n",
    "print(sorted(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aba995-fddb-4d33-97e8-3fccd628b897",
   "metadata": {},
   "source": [
    "<span style = \"color:blue\">Example 4:</span> Generating words without prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0dc4f10-5781-4b66-b222-a0b559d753a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['better', 'birth', 'direction', 'guess', 'hospital', 'machine', 'match', 'money', 'tell', 'zero']\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "for _ in range(10):\n",
    "    ans.append(char_level_model.generate_word(\"\"))\n",
    "print(sorted(ans))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
