{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9f06ec-855e-403b-b6da-89bd60e4a865",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59c396a0-4881-4fa7-8bac-f533b2164efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95e6a8-04f7-41e1-884c-7ba06d0d0fdc",
   "metadata": {},
   "source": [
    "Load a dataset of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c49d7707-3c1a-4c9c-8165-6e92fe64e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.txt\", 'r') as file:\n",
    "        raw_text = file.read()\n",
    "raw_text = raw_text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e75fcf-ed4f-40f7-9679-e054bdc43799",
   "metadata": {},
   "source": [
    "The vocabulary is built using the words in the given dataset\n",
    "\n",
    "The dataset is modified such that start (<span style=\"color:red\">\"\\<s\\>\"</span>) and end (<span style=\"color:red\">\"\\</s\\>\"</span>) tokens are wrapped around each sentence.\n",
    "\n",
    "The tokens are also added to the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5d479e2-1841-45fa-b0f4-5a215a9c11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentence_tokens(tokens, start_token, stop_token, punctuations):\n",
    "    \"\"\" Adds start and end tokens to the beginning and end of each sentence.\n",
    "    Args:\n",
    "        tokens (list): tokenized dataset\n",
    "        start_token (str)\n",
    "        stop_token (str)\n",
    "    Returns:\n",
    "        list: modified tokenized dataset\n",
    "    \"\"\"\n",
    "    result = [start_token]\n",
    "    for token in tokens:\n",
    "        result.append(token)\n",
    "        if token in punctuations:\n",
    "            result.append(stop_token)\n",
    "            result.append(start_token)\n",
    "    result.append(stop_token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97106180-0ec1-4c81-8866-272ec26e4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token, stop_token = \"<s>\", \"</s>\"\n",
    "punctuations = ['.', '!', '?']\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "dataset = add_sentence_tokens(tokenizer(raw_text), start_token, stop_token, punctuations)\n",
    "vocab = sorted(list(set(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85c5fd-c8b1-4afd-b116-6170a3915c96",
   "metadata": {},
   "source": [
    "The TokenLevelRNN class includes the implementation of the language model together with methods used in training the model and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0be2dc2-0b6c-485f-b80c-f9278f38d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLevelRNN(nn.Module):\n",
    "    \"\"\" RNN-based token level language model\n",
    "        The developed model generates sentences one word at a time\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, embedding_size, num_hidden_layers, hidden_layer_size, start_token, stop_token, tokenizer, punctuations):\n",
    "        nn.Module.__init__(self)\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_size)        \n",
    "        self.rnn = nn.RNN(embedding_size, hidden_layer_size, batch_first=True, num_layers= num_hidden_layers, dropout = 0.2)\n",
    "        self.fully_connected = nn.Linear(hidden_layer_size, self.vocab_size)\n",
    "        self.token_to_index_map = {vocab[i]:i for i in range(len(vocab))}\n",
    "        self.index_to_token_map = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_token = start_token\n",
    "        self.stop_token = stop_token\n",
    "        self.punctuations = \",.?!:'\"\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden = None):\n",
    "        \"\"\" Neural network forward pass\n",
    "        Args:\n",
    "            x(torch.Tensor): input data\n",
    "            hidden(torch.Tensor): hidden state\n",
    "        Returns:\n",
    "            tuple: logits and the last calculated hidden state\n",
    "            \n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        logits = self.fully_connected(output)\n",
    "        return logits, hidden\n",
    "        \n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\" Converts string to a list of tokens' indices in the vocabulary\n",
    "        Args:\n",
    "            text (str): text to be processed\n",
    "        Returns:\n",
    "            list: list of indices\n",
    "        \"\"\"\n",
    "        return [self.token_to_index_map[token] for token in text]\n",
    "\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\" Converts indices to corresponding tokens\n",
    "        Args:\n",
    "            indices(list): list of indices\n",
    "        Returns:\n",
    "            str: Decoded string\n",
    "        \"\"\"\n",
    "        return [self.index_to_token_map[index] for index in indices]\n",
    "\n",
    "    \n",
    "    def __generate_batch(self, data, seq_length, batch_size):\n",
    "        \"\"\" Randomly samples data and generate a batch\n",
    "        Args:\n",
    "            data: dataset to be sampled\n",
    "            seq_length(int): length of each sampled sequence\n",
    "            batch_size(int): batch size\n",
    "        Returns:\n",
    "            tuple: a batch of data and labels\n",
    "        \"\"\"\n",
    "        batch_start_indices = torch.randint(len(data) - seq_length, (batch_size,))\n",
    "        x = torch.stack([data[i:i+seq_length] for i in batch_start_indices])\n",
    "        y = torch.stack([data[i+1:i+seq_length+1] for i in batch_start_indices])\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "    def train_model(self, data, epochs, optimizer, criterion, seq_length = 50, batch_size = 32, loss_print_interval = 1000):\n",
    "        \"\"\" Trains the weigths of the model\n",
    "        Args:\n",
    "            data(str): training data\n",
    "            epochs: number of epochs\n",
    "            optimizer: e.g., torch.optim.Adam\n",
    "            criterion: e.g., nn.torch.CrossEntropyLoss\n",
    "            seq_length(int): length of each randomly sampled data sequence\n",
    "            batch_size(int): size of randomly sampled batch of data per each epoch\n",
    "            loss_print_interval (int): the interval to print the loss during training\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        data = torch.tensor(self.encode(data), dtype = torch.long)\n",
    "        for epoch in range(1, epochs+1):\n",
    "            self.train()\n",
    "            x_batch, y_batch = self.__generate_batch(data, seq_length, batch_size)\n",
    "            logits, _ = self.forward(x_batch)\n",
    "            loss = criterion(logits.view(-1, self.vocab_size), y_batch.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch == 1 or epoch % loss_print_interval == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    def __calculate_hidden_state(self, input_seq):\n",
    "        \"\"\" Calculates hidden state (which is used to generate the next character given a sequence)\n",
    "        Args:\n",
    "            input_seq(torch.Tensor): sequence of input\n",
    "        Returns:\n",
    "            torch.Tensor: last calculated hidden state\n",
    "        \"\"\"\n",
    "        if input_seq.nelement() == 0:\n",
    "            return None\n",
    "        _, hidden = self.forward(input_seq, None)\n",
    "        return hidden.squeeze(1)\n",
    "\n",
    "\n",
    "    def __convert_tokens_to_sentences(self, tokens):\n",
    "        \"\"\" Converts list of tokens to sentences\n",
    "        Args:\n",
    "            tokens (list): list of tokens\n",
    "        Returns:\n",
    "            str: the sentences formed by input tokens\n",
    "        \"\"\"\n",
    "        ans = []\n",
    "        for token in tokens:\n",
    "            if token in self.punctuations:\n",
    "                ans.append(token)\n",
    "            elif token in [self.start_token, self.stop_token]:\n",
    "                continue\n",
    "            else:\n",
    "                ans.append(\" \" + token)\n",
    "        return \"\".join(ans).strip().replace(\"' \", \"'\")\n",
    "\n",
    "\n",
    "    def generate_sentence(self, start_text = \"\", temperature = 1.0, expected_sentences_count = 5):\n",
    "        \"\"\" Generates sentences word by word given a start text. Uses softmax, and samples a word randomly\n",
    "        Args:\n",
    "            start_text(str): start_text of the expected paragraph\n",
    "            temperature(float): temperature scaling applied to logits \n",
    "                temperature<1 -> more confident sampling (peaky distribution)\n",
    "                temperature=1 -> normal sampling\n",
    "                temperature>1 -> more creative sampling (flatter distribution)\n",
    "        Returns:\n",
    "            str: generated sentences\n",
    "        \"\"\"\n",
    "        generated_sentences_count = 0\n",
    "        tokenized_start_text = self.tokenizer(start_text)\n",
    "        if not tokenized_start_text or tokenized_start_text[0] != self.start_token:\n",
    "            tokenized_start_text = [self.start_token] + tokenized_start_text\n",
    "        indices   = self.encode(tokenized_start_text)\n",
    "        input_seq = torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "        hidden = self.__calculate_hidden_state(input_seq[:,:-1])\n",
    "        self.eval()\n",
    "        while True:\n",
    "            current_index = input_seq[:,-1]\n",
    "            logits, hidden = self.forward(current_index, hidden)\n",
    "            probs = torch.softmax(logits/temperature, dim=-1)\n",
    "            predicted_index = torch.multinomial(probs, num_samples=1).item()\n",
    "            if self.decode([predicted_index])[0] == self.stop_token:\n",
    "                generated_sentences_count += 1\n",
    "                if generated_sentences_count == expected_sentences_count:\n",
    "                    break\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[predicted_index]])], dim=1)\n",
    "        tokens = self.decode(input_seq.squeeze().tolist())\n",
    "        return self.__convert_tokens_to_sentences(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba74ec6-d9cf-4192-b69b-ba8c7f9db89b",
   "metadata": {},
   "source": [
    "Creating an object of the TokenLevelRNN class (embedding size = 200; hidden layer size = 1024, num_hidden_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8eb64fb-8a82-4339-bd8e-1b67cd6d0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_level_model = TokenLevelRNN(\n",
    "                                    vocab = vocab, embedding_size = 200, num_hidden_layers = 2,\n",
    "                                    hidden_layer_size = 1024, start_token = start_token, \n",
    "                                    stop_token = stop_token, tokenizer = tokenizer, punctuations = \",.?!:'\"\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141cd9b9-115b-4769-ad5c-0d78f3210eb9",
   "metadata": {},
   "source": [
    "Defining optimizer and loss and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39a3047-8345-4185-9f89-25e151a3d3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.6943\n",
      "Epoch 100, Loss: 3.2874\n",
      "Epoch 200, Loss: 2.0825\n",
      "Epoch 300, Loss: 1.2689\n",
      "Epoch 400, Loss: 0.7497\n",
      "Epoch 500, Loss: 0.4755\n",
      "Epoch 600, Loss: 0.3710\n",
      "Epoch 700, Loss: 0.3199\n",
      "Epoch 800, Loss: 0.3100\n",
      "Epoch 900, Loss: 0.2862\n",
      "Epoch 1000, Loss: 0.2826\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(token_level_model.parameters(), 0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "token_level_model.train_model(\n",
    "                                data = dataset, epochs = 1000, optimizer = optimizer, criterion = criterion\n",
    "                                , seq_length = 50, batch_size = 256, loss_print_interval = 100\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aba995-fddb-4d33-97e8-3fccd628b897",
   "metadata": {},
   "source": [
    "<span style = \"color:blue\">Example 1:</span> Generating words without any starting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0dc4f10-5781-4b66-b222-a0b559d753a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dreamed a giant called hagrid came to tell me i was going to a school for wizards. when i open my eyes i'll be at home in, as if they're not wanted at home. he was looking over at harry as he spoke. crabbe and goyle chuckled. it, as if snape had started handing out sweets.\n"
     ]
    }
   ],
   "source": [
    "ans = token_level_model.generate_sentence(\"\", expected_sentences_count = 5)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd738c5-f612-4bbe-9041-2a8c23489de7",
   "metadata": {},
   "source": [
    "<span style = \"color:blue\">Example 2:</span> Generating sentences first of which starts with <span style = \"color:blue\"> \"Harry was sad because he could not remember\"</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8406b45c-44c0-4280-8de7-a186958088ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry was sad because he could not remember. finally he said slowly, so me, now. a braver man than vernon dursley would have quailed under the furious look hagrid now wouldn't be allowed to. bill norbert was about quirrell sometimes there was something else to see what he had done in the walls of books. it was only then that harry realized what was standing behind quirrell.\n"
     ]
    }
   ],
   "source": [
    "ans = token_level_model.generate_sentence(\"Harry was sad because he could not remember\", expected_sentences_count = 5)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67392d-08a6-4669-9810-e5f7553d2bdb",
   "metadata": {},
   "source": [
    "<span style = \"color:blue\">Example 3:</span> Generating sentences first of which is <span style = \"color:blue\"> \"Ron was excited about\"</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "008dc935-c492-40b2-b2f2-dc9996901bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ron was excited about the owls or the bludgers unless they crack my head open. don't worry, i'm going to drag harry away. sometimes, he'd never been more nervous, never, not even when he'd had to take a school report home to the dursleys saying that he'd somehow turned his teacher's wig blue. he kept his eyes fixed on the door. any second now, professor mcgonagall would come back and lead him to his doom.\n"
     ]
    }
   ],
   "source": [
    "ans = token_level_model.generate_sentence(\"Ron was excited about\", expected_sentences_count = 5)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56cf079-cfec-4c25-88db-42bbd245659f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
